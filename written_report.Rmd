---
title: "written_report"
author: "Gabriella Montalvo"
date: "2025-05-10"
output:
  pdf_document: default
  html_document: default
---

```{r setup, echo=FALSE, message=FALSE,include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(caret)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(jsonlite)
library(ggrepel)
library(MASS)
library(lubridate)
library(rpart)
library(glmnet)
library(randomForest)
library(zoo)
library(rpart.plot)
```

```{r, echo = FALSE, message=FALSE, warning=FALSE}

allegations <- read.csv('/Users/gabbym/Desktop/Applied Machine Learning/Civilian_Complaint_Review_Board__Allegations_Against_Police_Officers_20250310.csv')

head(allegations)

complaints <- read.csv('/Users/gabbym/Desktop/Applied Machine Learning/Civilian_Complaint_Review_Board__Complaints_Against_Police_Officers_20250310.csv')

head(complaints)

penalties <- read.csv('/Users/gabbym/Desktop/Applied Machine Learning/Civilian_Complaint_Review_Board__Penalties_20250310.csv')

head(penalties)

officers <- read.csv('/Users/gabbym/Desktop/Applied Machine Learning/Civilian_Complaint_Review_Board__Police_Officers_20250310.csv', stringsAsFactors = FALSE)

head(officers)
```

# Final Project: Predicting Disciplinary Action for Complaints against NYPD Officers
### Analyzing Differences Between Model Performances in Disciplinary Action Prediction

Find the corresponding R code and data at this link: https://github.com/gm3005/gm3005_Final

## I. Problem Statement

As an Ethnicity & Race studies major with work experience in incarceration education and gun violence prevention, I am interested in applying machine learning to identifying patterns in criminalization and incarceration in New York City. Where my previous research has concerned racial disparities in civilian arrests/summonses, I would like to shift my focus to discipline/penalty within the NYPD. The Civilian Complaint Review Board has ample data on complaints made against NYPD officers, along with the demographics of NYPD personnel. 

As police misconduct has taken an historic racial skew, I turn to the records of the CCRB to determine which factors--demographic, geographic, institutional, etc.--may have an influence on whether NYPD officers are disciplined for alleged wrongdoing. My aim is to conduct a comprehensive analysis robust enough to advise victims of police wrongdoing on the likelihood of receiving justice, should they choose to file a complaint with the CCRB. I do not seek to dissuade New Yorkers from reporting police misconduct, but to provide honest, realistic context with which potential complainants may make an informed decision. I do so using a classification tree and a random forest as my predictive models, assessing their relative performances as they relate to ensemble averaging and random feature selection. Ultimately, I incorporate a bagged tree to measure against the promising predictive power of the random forest model.   

## II. Highlight: Impressive Result

Where the random forest model outperformed the classification tree by a wide margin, the bagged tree performed the strongest of the three. While the random forest output a precision score of 0.97, a recall score of 0.85, and an F1 score of 0.91, the bagged tree boasted a precision score of 97%, a recall of 89%, and An F1 score of 92%. Their Kappas--0.8558 and 0.8751, respectively--suggest that both models are robust in their predictions, in which the predicted values of both agree with the actual values about over 85% of the time. This is especially impressive given the high variance and high correlation of bagged trees. For more on this comparison, turn to [SECTION WHATEVER]. 

Both the random forest and bagged tree presented victim race, victim age, reason for police contact, total complaints filed against a given officer, and precinct as the most influential factors in predicting whether an officer would face penalty. 

## III. Introduction: Data and Models 


```{r, echo = FALSE, warning=FALSE, message=FALSE}

# merging complaints df with allegations df by "Complaint.Id"
complaints_allegations <- merge(complaints, allegations, by = "Complaint.Id", all.x = TRUE)

# merging with penalties df on "Complaint.Id"
cap <- merge(complaints_allegations, penalties, by = "Complaint.Id")

# cleaning cap
  
## remove duplicates
cap <- cap %>% distinct()
officers <- officers %>% distinct()

## match column names
colnames(cap) <- gsub("\\.", "_", colnames(cap))  # replace dots with underscores
colnames(officers) <- gsub("\\.", "_", colnames(officers))

## convert dates to date format
date_cols <- c("As_Of_Date_x", "CCRB_Received_Date", "Close_Date", "As_Of_Date_y")
cap[date_cols] <- lapply(cap[date_cols], as.Date, format="%m/%d/%Y")

## missing values
cap[cap == ""] <- NA

## redundant columns
cap <- subset(cap, select = -c(As_Of_Date_y, Tax_ID_y, As_Of_Date_x))

## merge together
full <- cap %>%
  left_join(officers, by = c("Tax_ID_x" = "Tax_ID"))

```


```{r, echo = FALSE, warning=FALSE, message=FALSE}

# clean up MERGED dataset

## remove duplicates (Complaint_Id, Tax_ID_x, Allegation_Record_Identity)
full_clean <- full %>%
  distinct(Complaint_Id, Tax_ID_x, Allegation_Record_Identity, .keep_all = TRUE)

## drop redundant columns
full_clean <- subset(full_clean, select = -c(As_Of_Date.x, As_Of_Date.y))

## standardize column names
colnames(full_clean) <- gsub(" ", "_", colnames(full_clean))  # replace spaces with underscores
colnames(full_clean) <- gsub("__+", "_", colnames(full_clean)) # remove double underscores

## convert date columns to Date format
date_cols2 <- c("Last_Reported_Active_Date", "Non_APU_NYPD_Penalty_Report_Date", "APU_Closing_Date")
full_clean[date_cols2] <- lapply(full_clean[date_cols2], as.Date, format="%m/%d/%Y")

## missing values -> NA
full_clean[full_clean == ""] <- NA

## categorical variables to factors
factor_cols <- c("Borough_Of_Incident_Occurrence", "Location_Type_Of_Incident", "Outcome_Of_Police_Encounter",
                 "Officer_Rank_At_Incident", "Officer_Gender", "Officer_Race", "FADO_Type", "Allegation",
                 "CCRB_Allegation_Disposition", "NYPD_Allegation_Disposition", "Investigator_Recommendation")

full_clean <- full_clean %>%
  mutate(across(all_of(factor_cols), as.factor))

# reorder columns for readability
full_clean <- full_clean %>%
  arrange(Complaint_Id, Incident_Date, CCRB_Received_Date, Close_Date, Borough_Of_Incident_Occurrence,
         Precinct_Of_Incident_Occurrence, Location_Type_Of_Incident, Reason_for_Police_Contact,
         Outcome_Of_Police_Encounter, Allegation, FADO_Type, CCRB_Allegation_Disposition, 
         NYPD_Allegation_Disposition, Tax_ID_x, Officer_First_Name, Officer_Last_Name, Officer_Race, 
         Officer_Gender, Officer_Rank_At_Incident, Current_Rank, Active_Per_Last_Reported_Status)


head(full_clean)

# indexing dataset to focus on years of interest 

cutoff_date <- Sys.Date() - years(10)
ccrb10 <- full_clean %>% filter(Incident_Date >= cutoff_date)

```


```{r, echo = FALSE, message=FALSE, warning=FALSE}

# finding which columns are NA heavy with colSums(is.na(ccrb10)), removing them for pilot

colSums(is.na(ccrb10))

ccrb101 <- subset(ccrb10, select = -c(Victim_Alleged_Victim_Race_Ethnicity, Non_APU_NYPD_Penalty_Report_Date, APU_Plea_Agreed_Penalty, APU_Closing_Date, APU_Trial_Commissioner_Recommended_Penalty, NYPD_Allegation_Disposition, Board_Discipline_Recommendation, APU_Case_Status, CCRB_Received_Date, Close_Date, BWC_Evidence, Video_Evidence, Officer_Command_At_Incident, Allegation_Record_Identity, Current_Rank_Abbreviation, Current_Command, Shield_No)) # removing Ethnicity because too many NAs, overlaps with Race almost completely
 
 ## slimmed df down to 34 (potential) features

# interpreting NAs in Officer Penalty to mean no penalty
ccrb101$NYPD_Officer_Penalty[is.na(ccrb101$NYPD_Officer_Penalty)] <- "No penalty"

# checking more NAs with colSums(is.na(ccrb101)), filling NAs in gender, race, and age categories + Reason_For_Police_Contact 

## gender 
ccrb101$Victim_Alleged_Victim_Gender[is.na(ccrb101$Victim_Alleged_Victim_Gender)] <- "Missing"

## race 
ccrb101$Victim_Alleged_Victim_Race_Legacy_[is.na(ccrb101$Victim_Alleged_Victim_Race_Legacy_)] <- "Missing"

## age range 
ccrb101$Victim_Alleged_Victim_Age_Range_At_Incident[is.na(ccrb101$Victim_Alleged_Victim_Age_Range_At_Incident)] <- "Missing"

## reason for police contact 
ccrb101$Reason_for_Police_Contact[is.na(ccrb101$Reason_for_Police_Contact)] <- "Missing"

# using values from neighboring rows to fill NAs in borough, precinct, location type, hour, date, and rank
ccrb101$Borough_Of_Incident_Occurrence <- na.locf(ccrb101$Borough_Of_Incident_Occurrence)
ccrb101$Precinct_Of_Incident_Occurrence <- na.locf(ccrb101$Precinct_Of_Incident_Occurrence)
ccrb101$Location_Type_Of_Incident <- na.locf(ccrb101$Location_Type_Of_Incident)
ccrb101$Incident_Hour <- na.locf(ccrb101$Incident_Hour)
ccrb101$Incident_Date <- na.locf(ccrb101$Incident_Date)
ccrb101$Officer_Rank_At_Incident <- na.locf(ccrb101$Officer_Rank_At_Incident)

```


```{r, echo = FALSE, warning=FALSE, message=FALSE}

# (almost) final features

ccrb <- subset(ccrb101, select = c(Borough_Of_Incident_Occurrence, Location_Type_Of_Incident, Reason_for_Police_Contact, Officer_Rank_At_Incident, Officer_Days_On_Force_At_Incident, FADO_Type, Victim_Alleged_Victim_Age_Range_At_Incident, Victim_Alleged_Victim_Gender, Victim_Alleged_Victim_Race_Legacy_, Officer_Race, Officer_Gender, Total_Complaints, Precinct_Of_Incident_Occurrence, NYPD_Officer_Penalty))

ccrb <- data.frame(ccrb, stringsAsFactors = FALSE)

# renaming lengthy columns (victim age, race, gender) to avoid messy model outputs
names(ccrb)[names(ccrb) == "Victim_Alleged_Victim_Race_Legacy_"] <- "Victim_Race"
names(ccrb)[names(ccrb) == "Victim_Alleged_Victim_Age_Range_At_Incident"] <- "Victim_Age_Range"
names(ccrb)[names(ccrb) == "Victim_Alleged_Victim_Gender"] <- "Victim_Gender"
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}

#colSums(is.na(ccrb))

# cleaning up remaining NAs, so that na.omit() removes the least possible NAs
ccrb$Officer_Race <- as.character(ccrb$Officer_Race)
ccrb$Officer_Race[is.na(ccrb$Officer_Race)] <- "Missing"

ccrb$Officer_Gender <- as.character(ccrb$Officer_Gender)
ccrb$Officer_Gender[is.na(ccrb$Officer_Gender)] <- "Missing"

ccrb$Officer_Race <- as.character(ccrb$Officer_Race)
ccrb$Officer_Race[is.na(ccrb$Officer_Race)] <- "Missing"

ccrb$Total_Complaints <- as.character(ccrb$Total_Complaints)
ccrb$Total_Complaints[is.na(ccrb$Total_Complaints)] <- "NA filed"

ccrb <- na.omit(ccrb)

colSums(is.na(ccrb))

```

### A. Civilian Complaint Review Board Datasets

For the purposes of this project, I used four datasets created by the NYC Civilian Complaint Review Board, which logs complaints and facilitates disciplinary proceedings against NYPD officers with allegations of civilian misconduct. All of this data is available via the CCRB's "Data Transparency Initiative," which can be found at https://www.nyc.gov/site/ccrb/policy/data-transparency-initiative.page. 

With the goal of predicting disciplinary action for complaints against NYPD officers, I am merging four datasets (Complaints, Allegations, Penalties, and Officers) to construct a larger frame of reference for my model to pull insights from, one which includes not only the specifics of misconduct incidents and corresponding disciplinary action, but also officer demographics such as race, age, and ranking. 

The four datasets include the following: 

* complaints - logs complaints filed against the NYPD to the CCRB. Complaints can include multiple claims of misconduct (of the four categories--Force, Abuse of Authority, Discourtesy, and Offensive Language, collectively known as "FADO") from one instance. Included is also the status of the complaint and whether or not evidence exists to substantiate the complaint. 
* allegations - logs the individual allegations filed by CCRB complainants. Multiple allegations can correspond to the same complaint. Included are also descriptors of the officers against whom allegations are filed, including their days on the force and command at the time of the incident. Here you can find the race, gender, and age of victims making these allegations. 
* penalties - logs CCRB recommendations for discipline per complaint, followed by recommendations made by the Administrative Prosecution Unit for substantiated misconduct allegations. In NYPD.Officer.Penalty, one can find the final disciplinary decision. A great deal of this data is missing, suggesting that following through with proceedings is a lengthy and at time unfinished process that remains stagnant at the NYPD. 
* officers - logs pertinent demographic information like officer race, gender, age, rank, as well as corresponding complaint numbers. 

When merged (into a dataframe I named "ccrb"), the four datasets provide a wealth of insight on the confusing landscape of police accountability in New York City. Featured in the data are categorical, numeric, and ordinal variables across domains of incident context (which provide details of the police encounters that prompted the CCRB complaint), identity (demographics of both the complainant and the police perpetrator), and institutional structure (information about an officer's history in the force). The data gestures toward the complex relationships which emerge between police and civilians, weighed by structural and historic conditions that are difficult to quantify. As nonlinear interactions occur between features that compound on one another (like race and gender), sensitive and interpretable models are necessary. 

I narrowed the large scope of the data to the following features, taking great care eliminate as few "NAs" as possible:

* **Borough_Of_Incident_Occurrence** measures which borough each complaint is filed in. 
* **Precinct_Of_Incident_Occurrence** measures which precinct each complaint is filed in. This will be informative in relation to Incident_Date, as one can see which police captain presided over which precinct at any given time. 
* **Location_Type_of_Incident** measures whether a recorded complaint occured on the street, in a residence, on the subway, etc. 
* **Reason_for_Police_Contact** categorizes how the police-civilian interaction began, whether it be due to a "parking violation," a report, or an officer presuming some violation of a crime.
* **Officer_Rank_At_Incident** measures the office of an NYPD officer at the time the complaint was filed against them. 
* **Officer_Experience_Bin** categorizes how many years an officer has worked for the NYPD based on "Officer_Days_On_Force_At_Incident," arranging officers into buckets for every five years of experience. I engineered this for interpretability. 
* **FADO_Type** measures the type of allegation filed in a complaint——Force, Abuse of Authority, Discourtesy, and Offensive Language.
* **Victim_Alleged_Victim_Age_Range_At_Incident** catalogs the age-range of the victim filing the complaint. Those filing usually self-report their age, but I cannot say with certainty if that was the practice for older entries in the dataset.
* **Victim_Alleged_Victim_Gender** catalogs the gender of the victim filing the complaint. Those filing usually self-report their gender, but I cannot say with certainty if that was the practice for older entries in the dataset.
* **Victim_Alleged_Victim_Race_Legacy_** catalogs the race of the victim filing the complaint. Those filing usually self-report their race, but I cannot say with certainty if that was the practice for older entries in the dataset.
* **Officer_Race** catalogs the race of the officer whom the complaint is filed against. It has been obtained from a NYPD database of NYPD personnel demographics.
* **Officer_Gender** catalogs the gender of the officer whom the complaint is filed against. It has been obtained from a NYPD database of NYPD personnel demographics.
* **Total_Complaints** counts how many complaints have compiled against a specific officer in total. 

I chose to substitute NAs for "Missing" to flag potential instances of intentional data obfuscation. How could it benefit/harm an officer to omit essential demographic data like victim race, gender, or age range? Omitting information about the reason for police contact could also ease the penalty against an officer for a complaint. These "Missing" values are thus very intentional. 

I engineered a binary target variable, Penalty_Binary, from NYPD_Officer_Penalty, to streamline the prediction process to penalty (1) or no penalty (0). 

### B. Exploratory Visualizations 

With an end goal of predicting disciplinary action for complaints against NYPD officers, my more immediate aim is to engineer a target variable that is readable and accessible for my presumed audience--those interested in filing a claim against the NYPD through the CCRB. Considering my audience may just be getting acquainted with the system and its patterns of punishment, I'm including some relatively plain visualizations to get more comfortable with the data. 

```{r, echo = FALSE, warning=FALSE, message=FALSE}

# checking levels of officer penalties; a lot of different variety (many levels to category, could be problematic)
unique(ccrb$NYPD_Officer_Penalty)

# setting variable to factor
ccrb$NYPD_Officer_Penalty <- as.factor(ccrb$NYPD_Officer_Penalty)

# creating target binary variable, in case many levels of categorical response are too unwieldy
ccrb$Penalty_Binary <- ifelse(ccrb$NYPD_Officer_Penalty == "No penalty", 0, 1)

# updating dataframe so target variable is only the binary Penalty_Binary variable
#ccrb <- subset(ccrb, select = -NYPD_Officer_Penalty)


# categorizing officer years of experience for readability (instead of days on workforce)
ccrb$Officer_Experience_Bin <- cut(ccrb$Officer_Days_On_Force_At_Incident, 
                                  breaks = c(0, 365, 1825, 3650, Inf), 
                                  labels = c("0-1yr", "1-5yrs", "5-10yrs", "10+yrs"))

# colSums(is.na(ccrb)) ; some missing data in experience bins 
ccrb$Officer_Experience_Bin <- na.locf(ccrb$Officer_Experience_Bin) # using neighboring values to fill in blanks

```

```{r, echo = FALSE, warning=FALSE, message=FALSE}

# convert all character strings to factors
ccrb[sapply(ccrb, is.character)] <- lapply(ccrb[sapply(ccrb, is.character)], 
                                       as.factor)

ccrb$Penalty_Binary <- as.factor(ccrb$Penalty_Binary)

```

```{r, include=TRUE}

# 1. Distribution of Penalty_Binary
ggplot(ccrb, aes(x = Penalty_Binary, fill = Penalty_Binary)) +
  geom_bar() +
  labs(title = "Distribution of Penalty Binary", x = "Penalty (1 = Yes, 0 = No)", y = "Count") +
  theme_minimal()

# 2. FADO Type vs. Penalty
ggplot(ccrb, aes(x = FADO_Type, fill = Penalty_Binary)) +
  geom_bar(position = "fill") +
  coord_flip() +
  labs(title = "Proportion of Complaints Leading to Penalty by FADO Type", x = "FADO Type", y = "Proportion") +
  theme_minimal()

# 3a. Officer Race vs. Penalty
ggplot(ccrb, aes(x = Officer_Race, fill = Penalty_Binary)) +
  geom_bar(position = "fill") +
  coord_flip() +
  labs(title = "Proportion of Officers Receiving Penalties by Race", x = "Officer Race", y = "Proportion") +
  theme_minimal()

#3b. Officer Gender vs. Penalty

ggplot(ccrb, aes(x = Officer_Gender, fill = Penalty_Binary)) +
  geom_bar(position = "fill") +
  labs(title = "Proportion of Officers Receiving Penalties by Gender", x = "Officer Gender", y = "Proportion") +
  theme_minimal()

# 4. Office Years of Experience vs. Penalty
ggplot(ccrb, aes(x = Officer_Experience_Bin, fill = Penalty_Binary)) +
  geom_bar(position = "fill") +
  coord_flip() +
  labs(title = "Proportion of Officers Receiving Penalties by Years of Experience", x = "Officer Race", y = "Proportion") +
  theme_minimal()

# 5. Boroughs vs. Penalty
ggplot(ccrb, aes(x = Borough_Of_Incident_Occurrence, fill = Penalty_Binary)) +
  geom_bar(position = "fill") +
  coord_flip() +
  labs(title = "Proportion of Complaints Leading to Penalty by Borough", x = "Borough", y = "Proportion") +
  theme_minimal()

# 6. Distribution of NYPD_Officer_Penalty
top_penalties <- ccrb %>%
  count(NYPD_Officer_Penalty, sort = TRUE) %>%
  top_n(10, n)

# 7. Plot the top 10 penalties
ggplot(top_penalties, aes(x = reorder(NYPD_Officer_Penalty, n), y = n, fill = NYPD_Officer_Penalty)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Top 10 NYPD Officer Penalties", x = "Penalty Type", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")

```

Plot 1: Loosely, it appears as though complaints result in more penalties than not. That said, this histogram does not distinguish between levels of discipline (redacting vacation days vs. removal from NYPD. 

Plot 2: FADO (Force, Abuse of Authority, Discourtesy, and Offensive Language) is how the CCRB categorizes allegations filed against the NYPD. Complaints citing use of force result in penalty the least at a proportional rate of under 55%. Abuse of authority appears to fall just under 60%, with Untruthful Statement at a bit above 60%. Offensive language appears to result in discipline the most, at a rate of about .65. 

Plot 3a (Race): Complaints against indigenous officers appear to result in penalty the least (less than 30%). It is worth noting that the indigenous population in New York City is smaller than that of Black, Hispanic, Asian, and White communities in NYC. Asian and Hispanic appear to fall along the same proportional rate, with about 60% of complaints resulting in discipline. Black and White officers fall around the same rate, both with over 60% of complaints resulting in discipline. 

Plot 3b (Gender): Trans and Gender Non-Conforming officers appear to receive more penalties proportionally than male- and female-identified officers. That said, we can assume the population of TGNC officers to be much smaller than that of male or female officers. Complaints filed against female officers result in disciplinary action the least, at a proportional rate of about 55%. Complaints filed against male officers result in disciplinary action at a higher proportion, about 60%. 

Plot 4: It appears more penalties are levied against more senior officers. I am quantifying seniority as 5+ years of experience on the force, with 5-10 years and 10+ years of experience have similar rates of penalty. Where officers of 1-5 years of experience tend to receive penalty at a rate above 50% (but still less than "senior" officers at about 60%), new officers with less than one year of experience receive penalties at a less than 50% rate. 

Plot 5: It appears complaints filed outside the metropolitan area are more likely to result in disciplinary action, proportionally. Complaints filed in Staten Island, Queens, Manhattan, and Brooklyn appear to hover just under a 60% penalty rate. The Bronx, however, appears to have the lowest proportion of complaints leading to penalty (likely just under 55%). 

Plot 6: As seen in the Top 10 NYPD Officer Penalties bar chart, officers with allegations/complaints of misconduct most often receive no penalty, followed (with large distance) by a pending decision from the Administrative Prosecution Unit, which is involved with more serious NYPD offenses. A pending decision (as of the week of March 9, 2025) suggests that there is a great deal of stagnation in the officer disciplinary process. Formalized training follows closely behind; it entails a mandatory review of NYPD standards similar to HR instruction. 

### C. Model Selection

To predict disciplinary outcomes from this wealth of ordinal, categorical, and numeric data, I turned first to the classification tree, then to the random forest. Classification trees and random forests are well-suited for data this complex, as they are apt for categorical data, can capture nonlinear interactions between features, and are robust to skew and outliers. While trees offer interpretability for single decisions, random forests deliver higher accuracy and generalization by averaging over many deep trees trained on bootstrapped samples. 

```{r, echo = FALSE, warning=FALSE, message=FALSE}

# using caret to split the data (80% training, 20% testing)

set.seed(123) # for reproducibility
                  
trainIndex <- createDataPartition(ccrb$Penalty_Binary, p = 0.8, list = FALSE)
train_data <- ccrb[trainIndex, ]
test_data <- ccrb[-trainIndex, ]

#colSums(is.na(train_data))

# ensuring categorical variables are all factors: 
train_data[sapply(train_data, is.character)] <- lapply(train_data[sapply(train_data, is.character)], 
                                       as.factor)
test_data[sapply(test_data, is.character)] <- lapply(test_data[sapply(test_data, is.character)], as.factor)

# shortening variable names to make trees more readable
# Example renaming for clarity

names(train_data) <- gsub("Precinct_Of_Incident_Occurrence", "Precinct", names(train_data))
names(test_data) <- gsub("Precinct_Of_Incident_Occurrence", "Precinct", names(test_data))

names(train_data) <- gsub("Reason_for_Police_Contact", "PoliceContact", names(train_data))
names(test_data) <- gsub("Reason_for_Police_Contact", "PoliceContact", names(test_data))

names(train_data) <- gsub("Officer_Rank_At_Incident", "Rank", names(train_data))
names(test_data) <- gsub("Officer_Rank_At_Incident", "Rank", names(test_data))

names(train_data) <- gsub("Officer_Experience_Bin", "Experience", names(train_data))
names(test_data) <- gsub("Officer_Experience_Bin", "Experience", names(test_data))

names(train_data) <- gsub("Location_Type_Of_Incident", "Location", names(train_data))
names(test_data) <- gsub("Location_Type_Of_Incident", "Location", names(test_data))

names(train_data) <- gsub("Borough_Of_Incident_Occurrence", "Borough", names(train_data))
names(test_data) <- gsub("Borough_Of_Incident_Occurrence", "Borough", names(test_data))

```

#### Classification Tree

Given that I am predicting a qualitative response rather than a quantitative one, I am choosing to use a classification tree (rather than a regression tree). Decision trees are relatively interpretable and can intake both categorical and numerical data. I should note that they can be prone to overfitting, at times capturing more noise than true patterns in the data. 

```{r, echo = FALSE, include = TRUE}

# training classification tree model 
tree <- rpart(Penalty_Binary ~ Borough + Location + PoliceContact + Rank + Experience + FADO_Type + Victim_Age_Range + Victim_Gender + Victim_Race + Officer_Race + Officer_Gender + Total_Complaints + Precinct, data = train_data, method = "class")

# plotting the tree
rpart.plot(
  tree,
  under = TRUE,
  cex = 0.5
)

printcp(tree) # display the results
summary(tree)

# make prediction
tree_pred <- predict(tree, test_data, type = "class")

#evaluate
confusionMatrix(tree_pred, as.factor(test_data$Penalty_Binary), mode = "prec_recall")
```

My first attempt at a classification tree revealed some valuable insights, substantiated by very little predictive power. The root node is Victim_Race (whether it is "American Indian/Missing" or not), suggesting that race is highly significant in predicting whether a complaint leads to disciplinary action. It appears that if a victim's race is American Indian or Missing, the prediction is 1 (penalty) with 100% probability (as indicated by the green node). This outcome is especially interesting considering the demographic distribution of NYC, which bears larger Black, White, Hispanic, and Asian populations than indigenous population. Complaints involving indigenous Americans in New York City appear to always result in discipline--a potential artifact of small sample size. A penalty is highly likely. Following splits occured on the Precinct variable, the PoliceContact variable, then the Victim Age variable. Ultimately, a Kappa of 0.3309 indicates little to no agreement between this model's predictions and actual values, and thus a poor model performance. Given that my this tree could have overfit the data, I next attempted pruning and cross validating a classification tree. 


```{r, echo = FALSE, include=TRUE}

set.seed(234)

# trying 10-fold CV
train_control <- trainControl(method = "cv", number = 10)

# training classification tree with CV
cvtree <- train(as.factor(Penalty_Binary) ~ Borough + Location + PoliceContact + Rank + Experience + FADO_Type + Victim_Age_Range + Victim_Gender + Victim_Race + Officer_Race + Officer_Gender + Total_Complaints + Precinct, data = train_data, method = "rpart", trControl = train_control, tuneLength = 10)

# model results
print(cvtree)

# plot tree (cv)
#plot(cvtree$finalModel)
#text(cvtree$finalModel, use.n = TRUE, cex = 0.7)

# Cleaner and more readable plot
rpart.plot(
  cvtree$finalModel,             
  type = 2,                 
  extra = 104,              
  under = TRUE,            
  box.palette = "GnBu",     
  fallen.leaves = TRUE,     
  cex = 0.5,                
  tweak = 1.2,             
  compress = TRUE,          
  varlen = 12,             
  faclen = 10              
)


# evaluate cross-validated (unpruned) model
cvtree.pred <- predict(cvtree, newdata = test_data)
confusionMatrix(cvtree.pred, as.factor(test_data$Penalty_Binary), mode = "prec_recall")

```


```{r, echo = FALSE, include=TRUE}

#check the best complexity parameter
best_cp <- cvtree$bestTune
print(best_cp)

#prune the tree using the best cp value
pruned_tree <- prune(cvtree$finalModel, cp = best_cp$cp)

#plot pruned tree
#plot(pruned_tree)
#text(pruned_tree, use.n = TRUE, cex = 0.7)

rpart.plot(
  pruned_tree,              
  type = 2,                 
  extra = 104,              
  under = TRUE,             
  box.palette = "GnBu",     
  fallen.leaves = TRUE,     
  cex = 0.5,                
  tweak = 1.2,              
  compress = TRUE,          
  varlen = 12,              
  faclen = 10  
)

# evaluate pruned model, had to work arround some errors ("Error in eval(predvars, data, env) :    object 'Borough_Of_Incident_OccurrenceBrooklyn' not found.")
test_data1 <- model.matrix(~ . -1, data = test_data) %>% as.data.frame()
p.cvtree.pred <- predict(pruned_tree, newdata = test_data1, type = "class")
confusionMatrix(p.cvtree.pred, as.factor(test_data1$Penalty_Binary), mode = "prec_recall")

```

The cross-validated and pruned tree yielded identical results with marginal improvement in Kappa, precision, and recall. At a cursory glance, Victim Race remained the most significant in predicting whether or not discipline occurs, this time splitting at "Missing." If the victim race is missing, the tree predicts penalty (1) with 100% certainty, likely due to sparse cases.  Where Victim Race is not missing (the left subtree), the first split occurs at gender "missing." We can continue down the tree to where gender is known. Where gender is known, the next split occurs at Precinct, with the 60th Precinct performing at a high penalty rate under these conditions. As Precinct delineation continues, another key split occurs at reason for police contact, in which incidents initiated during moving violations have higher likelihoods toward penalty, whereas instances of police misconduct at demonstrations (PoliceContact == Demonstration) have a lower likelihood of resulting in penalty. 

Where Victim Race is missing (the right subtree), the first split occurs at FADO Type, where use of force by a police officer is a major path to penalty. Further precinct-specific splits occur, with the next key splits at reason for police contact and location of incident occurrence; complaints issued while already in police custody bear higher penalty likelihoods, where those issued for incidents occurring in public space (highly visible locations) also improve likelihood of penalty. Another key split occurs at Officer Rank, in which accused officers maintaining the rank of Sergeant are less likely to receive disciplinary action after alleged misconduct. 

As for the confusion matrix output: a precision score of 0.60, a recall of 0.45, and an F1 score of 0.51 show minimal improvement from the first decision tree. Both the cross-validated tree and the pruned tree predict no penalty (0) correctly about 60% of the time, correctly classifying actual 0s only 45% of the time, and missing a significant number of actual 0s with moderate confidence. 

A Kappa of 0.2515 shows some agreement between actual and predicted values, but not substantial. Clearly, the classification tree struggled to model data this complex. 


### Random Forest

A random forest uses decision/classification trees as building blocks to construct a more powerful prediction model. The random forest proved to enhance accuracy and robustness, as each tree was trained on a random subset of the data. 

```{r, echo = FALSE, include=TRUE}

## using matrix because randomForest() cannot use factors that exceed 53 levels

# accounting for too many factor levels by converting Precinct to numeric
set.seed(234)
train_data$Penalty_Binary <- as.factor(train_data$Penalty_Binary) # making sure Penalty_Binary is a factor

# convert Precinct to numeric (too many unique levels)
train_data$Precinct <- as.numeric(as.factor(train_data$Precinct))
test_data$Precinct <- as.numeric(as.factor(test_data$Precinct))

train_features <- train_data %>% dplyr::select(c(Borough, Location, PoliceContact, Rank, Victim_Age_Range, Victim_Gender, Victim_Race, Officer_Race, Officer_Gender, Total_Complaints, Precinct, Experience, Penalty_Binary))

test_features <- test_data %>% dplyr::select(c(Borough, Location, PoliceContact, Rank, Victim_Age_Range, Victim_Gender, Victim_Race, Officer_Race, Officer_Gender, Total_Complaints, Precinct, Experience, Penalty_Binary))


# training random forest model (not with matrix)
rf <- randomForest(Penalty_Binary ~., 
                  data = train_features,
                  method = "rf", 
                  trControl = train_control, 
                  importance = TRUE, 
                  do.trace = 100)

# print
print(rf)

# feature importance
varImpPlot(rf)

# predict on test data
rf_pred <- predict(rf, newdata = test_features)

# Confusion Matrix
confusionMatrix(rf_pred, as.factor(test_features$Penalty_Binary), mode = "prec_recall")

write_csv(ccrb, "/Users/gabbym/Desktop/Applied Machine Learning/ccrb.csv")
```

The MeanDecreaseAccuracy plot measures how much model accuracy decreases when a variable is excluded, while the MeanDecreaseGini plot measures how each variable contributes to the homogeneity of nodes in the resulting random forest. That said, Reason_for_Police_Contact proved to be the most important in both plots, contradicting our previous models' evaluation of victim race as the most prevalent feature. The MeanDecreaseAccuracy plot presents an officer's total complaints and victim race next most important features, whereas the MeanDecreaseGini plot follows reason for police contact with total complaints and precinct of incident occurrence. Both plots classify officer gender as the least important in determining a disciplinary outcome. 

As for the confusion matrix, the random forest proceeded with a precision score of 0.97, a recall score of 0.85, and an F1 score of 0.91. In other words, when the random forest predicts 0, it is correct about 97% of the time, correctly identifying about 85% of actual 0s. This model definitely has the better predictive power than the classification tree. 

## IV. Model Comparison: Evaluating Random Forest Outperformance 

The breadth of the performance gap between the classification tree and the random forest warranted further investigation. I aimed to locate exact where the jump in performance occurred, narrowing my search to ensemble averaging and random feature selection--both key characteristics of the random forest model. To my surprise, predictive power appeared to lie in ensemble averaging, as a bagged tree outperformed the random forest. For this reason, my model recommendation is inconclusive. 

### Matrix Evaluation

To better understand model performance, I offer the confusion matrices of both the single classification tree and the random forest. For its minor improvements from our first classification tree, I used the matrix corresponding with the pruned tree. 

> Comparing Confusion Matrices

```{r, echo = FALSE, include=TRUE}

# comparing confusion matrices
cat("Classification Tree:\n")
confusionMatrix(p.cvtree.pred, test_data$Penalty_Binary, mode = "everything")

cat("Random Forest:\n")
confusionMatrix(rf_pred, test_data$Penalty_Binary, mode = "everything")

```

The classification tree predicts no penalty (0) correctly about 60% of the time, correctly classifies actual 0s only 45% of the time, and misses a significant number of actual 0s with moderate confidence. Its predicted values agree with the actual values only about 25% of the time. 

The randomForest predicts no penalty (0) correctly about 97% of the time, correctly classifying actual 0s about 85% of the time. Its predicted values agree with actual values at a rate of about 85%. 

### Variable Importance 

> Comparing Variable Importance

```{r, echo = FALSE, include=TRUE}

# variable importance for rf
varImpPlot(rf, n.var = 10)

# variable importance for classification tree
tree_importance <- pruned_tree$variable.importance
top_10 <- sort(tree_importance, decreasing = TRUE)[1:10]
barplot(top_10,
        main = "Top 10 Variable Importances - Classification Tree",
        col = "skyblue",
        las = 2,
        cex.names = 0.8)

```

Variable importance quantifies how much a model relies on a variable to make accurate predictions. The randomForest model calculates variable importance in two ways: with a Mean Decrease Accuracy plot and a Mean Decrease Gini plot. The MeanDecreaseAccuracy plot measures how much model accuracy decreases when a variable is excluded, while the MeanDecreaseGini plot measures how each variable contributes to the homogeneity of nodes in the resulting random forest.

A single classification tree, on the other hand, is highly variable and subject to change based on the arrangement of data in the training sample. As it is built hierarchically, the tree is sensitive to the order in which variables are processed. At each node, the tree selects the single best split, which reduces impurity the most at a given moment, not over the tree in its entirety. The top split determines every split that follows, meaning that once the top split is chosen, the tree will not reconsider any other splits. One misclassified record can thus shift the top split and downstream branches of a single tree. Thus, the top variable importances featured above are subject to change given a rearrangement of the training data. 

The highly variable tree finds importance in Victim Race, Victim Gender, Victim Age, Precinct, and Reason for Police Contact, splitting at various values of each variable. These importances reflect the tree's specific structure, not necessarily broader patterns in the data. A classification tree gives importance = 0 to any variable it didn't use. A variable's absence does not necessarily mean it is unimportant. 

When processing the data, random forests force each split to consider only a subset of the predictors; a good amount of splits will thus occur without accounting for the strongest predictor, giving other predictors a chance--this process is known as random feature selection. As the forest ensembles multiple trees and decorrelates them by considering subsets of predictor for each tree, it makes the average of the resulting trees less variable and hence more reliable. We can approach Reason for Police Contact, Total Officer Complaints, Victim Race, Victim Age, and Precinct as variables with real bearing on disciplinary outcomes, as the randomForest's strong predictive power and aggregatory method provides more reliability than a single tree. 

Note how the important variables of the single classification tree take one value. A single tree splits on Precinct 60, for instance, missing the full regional pattern of disciplinary action for one promising datapoint. In contrast, the random forest model is able to appreciate systematic patterns across variables like Precinct. See the marginal distribution of the partial dependence plot below:

```{r, echo = FALSE, include=TRUE}

library(pdp)

# marginal effect of precinct on penalty prediction
partialPlot(rf, train_features, x.var = "Precinct", which.class = 1)

```

Precincts in the higher ranges (between 60 and 80) have higher average penalty predictions (~0.36+) than those in the lower numbers. A single tree could not capture this pattern in its variable importance plot. 

### Ensembling vs. Random Feature Selection 

As we compare variable importances, we must recall that a single classification tree does not measure any average across several iterations of predictions. The tree is but an isolated  Below is an attempt at Bootstrap Aggregating--also known as bagging--to train multiple full trees on repeated samples from the same training data. Let's see how bagging our classification tree compares to the randomForest model. Strong performance will will indicate that the tree failed for its singularity, not its lack of randomness. 

```{r, echo = FALSE, include=TRUE}

# attempting to bag

n_predictors <- ncol(train_data) - 1  # Penalty_Binary is the target

bagged_tree <- randomForest(
  Penalty_Binary ~ ., 
  data = train_features, 
  mtry = n_predictors,     # Use all features at every split (bagging)
  ntree = 500,             # Number of trees
  importance = TRUE
)

# evaluate with confusion matrix

bagged_preds <- predict(bagged_tree, newdata = test_features)
confusionMatrix(bagged_preds, as.factor(test_features$Penalty_Binary), mode = "everything")

# variable importance plot

varImpPlot(bagged_tree, main = "Variable Importance - Bagged Tree")

```
The bagged tree predicts no penalty (0) correctly about 97% of the time (very few false positives), correctly classifying actual 0s about 89% of the time (high true positive rate). An F1 score of 92% is about 2% higher than that of the randomForest, suggesting that the bagged tree has an even better ability to accurately classify both positive and negative cases by combining precision and recall into a single value. Its predicted values agree with actual values at a rate of about 88%. This outcome is incredibly surprising! The single decision tree was thus limited by high variance. Ensemble averaging--even without random feature selection--significantly stabilizes and improves model quality. Perhaps the data benefits more from variance reduction (bagging) than from decorrelation of predictors (random feature selection). This may also indicate that important features are few and strong, and the random forest adds noise by randomly subsetting the data. 

## V. Risks/Limitations

### Case-Based Comparison

Comparing the predictive abilities of the random forest and bagged tree further highlights the similarity in their predictive power. 

```{r, echo = TRUE}


# 1. Sample 3 random rows from the test set
set.seed(123)
example_rows <- test_features[sample(nrow(test_features), 3), ]

# 2. Align factor levels with training set
for (col in names(train_features)) {
  if (is.factor(train_features[[col]])) {
    example_rows[[col]] <- factor(example_rows[[col]], levels = levels(train_features[[col]]))
  }
}

# 3. Predict with Random Forest
rf_classes <- predict(rf, newdata = example_rows)
rf_probs <- predict(rf, newdata = example_rows, type = "prob")[, "1"]

# 4. Predict with Bagged Tree
bagged_classes <- predict(bagged_tree, newdata = example_rows)
bagged_probs <- predict(bagged_tree, newdata = example_rows, type = "prob")[, "1"]

# 5. Combine into a comparison table
comparison <- data.frame(
  Actual = example_rows$Penalty_Binary,
  RF_Pred = rf_classes,
  RF_Prob_1 = round(rf_probs, 3),
  Bagged_Pred = bagged_classes,
  Bagged_Prob_1 = round(bagged_probs, 3)
)

print(comparison)

```

Based on the comparison table, it appears both the random forest and the bagged tree correctly predict a penalty (1) with high confidence for the first random entry. The bagged tree is more confident than the random forest. For the second random entry, both models predicted penalty (1), though the actual outcome resulted in no penalty (0). With moderate confidence from both models, one can assume this case was pretty ambiguous. For the third and final random entry, both models correctly predicted no penalty (0) with high confidence, suggesting that the features of this case in tandem were clearly indicative that there would be no disciplinary action. Perhaps the complainant was of a race with a corresponding low likelihood of disciplinary action for police perpetrators of misconduct. 

One must not get too excited about the high performance of the bagged tree, however. When bagging a large number of trees, it is no longer possible to represent the resulting statistical learning procedure using a single tree, thus obscuring feature importance. Bagging therefore improves prediction accuracy at the expense of interpretability. For this reason, I am skeptical of the variable importance plots of the bagged tree. The course's suggested textbook, An Introduction to Statistical Learning, states the following on bagging decision trees: 

"When building these decision trees, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors. The split is allowed to use only one of those m predictors. A fresh sample of m predictors is taken at each split...most or all of the trees will use [a] strong predictor in the top split. Consequently, all of the bagged trees will look quite similar to each other. Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities. In particular, this means that bagging will not lead to a substantial reduction in variance over a single tree in this setting." (321)

The high performance of the bagged tree thus could be due in part to high correlation. The common importance of variables in both the bagged tree and the random forest is relevant nonetheless. Reason for police contact, victim race, total officer complaints, precinct, and victim age range are pivotal in determining whether police officers are disciplined. 

In all, the impressive performance of the bagged tree suggests the strong performance of the random forest model is due in large part to its ensemble averaging, perhaps more than its random feature selection. 

## VI. Conclusion 

In seeking to explain the difference in performance between the classification tree and the random forest, I happened upon a surprising result--tree performance can vastly improve and even surpass a random forest in predictive power through bagging. I am hesitant to select the bagged tree as my final model, as bagged trees are highly correlated with minimally reduced variance by virtue of aggregated bootstrapping. For this reason, I will maintain my intitial assessment of the random forest as the best model, as its random feature selection ensures against variable overlap and reduces variance. I will attribute the impressive performance of the bagged tree to deepen my understanding of why the random forest succeeds--ensemble averaging proves instrumental in handling data as complex as that of the CCRB. 

Though I am apprehensive about the predictive potential of the bagged tree, I take its variable importance plot as confirmation of what the random forest found: reason for police contact, victim race, victim age, precinct, and number of complaints filed against a given officer are key features in indicating whether an officer will receive a penalty or not. 

With these factors in mind, I feel confident advising potential CCRB complainants to consider their social positioning and precinct before filing their complaint against the NYPD, with complaints in the 60s-80s (in Brooklyn) are more likely to result in disciplinary action.

